更新： 這幾天用plaidmlngraphtensorflow的方式 效能慘不忍睹 後來又回去試rocm 認真看了一遍官網的教學 才發現要用docker的環境@@ 少了這個步驟直接用tensorflowrocm也可以跑 但效能會讓你哭出來 不過最後結果還不錯啦 至少在我目前訓練的速度上跟1060都可以打平 有想要用amd跑tensorflow的也歡迎討論XD 最近在做gan的專題 剛好手邊有一張amd rx580 想說就來玩一下linuxrocm 不得不說 用amd顯卡做深度學習 真的在搞自己XD 光裝系統驅動 就耗了快一天... 更別提後來又重裝好幾次XD 而重點是裝好後跑training的效率 大概只有 gtx 1060的1/3... 這邊沒有詳細數據 因為測完當下真的太生氣 直接把linux砍了XD 我自己感覺是優化的問題 跑model的當下使用率很低 大家有興趣可以玩玩看 但amd的深度學習之路離nv還是有蠻大一段距離 順帶一提 放棄rocm後 發現可以用 plaidmlngraphtensoflow 不過這也是一條不歸路... : 是說現在 AMD RoCm 的原始碼已經被合併進 TensorFlow 官方的 codebase 裡 : 雖然說還是被稱為 community support build : https://github.com/tensorflow/tensorflowcommunitysupportedbuilds : 不過穩定性會不會已經有所提昇了 (?) : 不知版上有沒有勇者嘗試過呢 : 這年頭做 Deep Learning 時 GPUs 用的越來越兇 : 真希望多點競爭趕快降價 XD : 引述《exeex (人非腎鹹)》之銘言： : : 引述《geminitw (geminitw)》之銘言： : : : 目前有在考慮購置第二台機器跑 DL/ML : : : 但因為... 2080ti 價格偏高 也不知道哪個硬體版本穩定 : : : 加上 7nm GG 盛名 想說 GG 7nm 產能會被爛蘋果影響 : : : 乾脆幫 AMD GPU 7nm 捧個場. : : : 只是不知道 tensorflow based on AMD RoCm 目前跑起來的狀況/效能如何? : : : 請各位前輩指點迷津... 或者 推薦 2080ti 的版本也很可以 : : : (啥雪花? 金平? 太陽花? Turbo? 很亂... 我只是要跑 ML/DL) : : : 感謝 : : http://blog.gpueater.com/en/2018/04/23/00011_tech_cifar10_bench_on_tf13/ : : https://imgur.com/JMx1csU : : 雖然這資料有點舊，單就效能論，是有資格和nvidia一戰啦。 : : 但我覺得，要買vega來跑你還要加上debug的風險和時間。 : : 划不划算就看你認為這時間佔多少成本。 : : 有的人就喜歡嘗試研究，時間不佔成本。那vega就划算 : : 有的人就喜歡穩，不喜歡花時間在這種沒意義的除錯上。那就nvidia划算。 : : 以下真香評論： : : nvidia要保持領先也不是那麼容易的事情 : : 若要問AMD的ROCm在做啥 : : 基本上就是把nvidia cuda的API照抄一遍 : : cuda有什麼function我就改個名字，設計一個一毛一樣的function : : 然後再收錄進我大ROCm的Library中 : : 最後再設計一個自動更名程式，把原本寫給cuda的程式，自動改編為ROCm的程式 : : ( https://github.com/ROCmDeveloperTools/HIP ) : : 現在這個ROCm版的tensorflow，就是這樣搞出來的。 : : 以上面的測試報告看來，Vega 12nm 的效能是有達到能與nvidia一戰的水準 : : 就只差可靠度，以及軟體支援速度(什麼時候才會有官方pytorch??)。 : : 如果這年度Vega 7nm發售，且可靠度、支援速度能有所提升的話，那是挺值得買的。 : : 再搭配上AMD的HBCC記憶體技術，拿內存當顯存用，直接讓你突破16GB顯存容量限制。 : : 這點簡直令所有做AI的人垂涎三尺。 : : 但目前HBCCROCm應該還沒成熟 : : ( https://github.com/RadeonOpenCompute/ROCm/issues/525 ) : : 註： : : 我是有一張Vega 56啦，之前買來挖礦，但目前拿來打電動比較實在。 : : 跑實驗還是用1080ti。用rocm乾脆用colab算了...不想花一堆時間去搞AMD的。環境。train好再把model拉回來。試著搞了一整天之後我的感想只有幹他媽的rocm。AMD這方面真的輸太多了...。也可能是tensorflow支持度差。聽說跑CNN比N卡效率還高。另外還有更深坑的，在黑頻果上面用XDD~。雙精度通常是跑什麼啊?我記得N卡半精度強  A卡雙精。度強。搞這個還用AMD的GPU是抖M吧。買一張N卡 解決！ODO。學校實驗室10幾張2080在跑又快又穩。拉基amd。ipc提升15% ，顯卡跑分和2060平手。一定是app問題。沒有cudnn跟cuda用應該是差很多吧